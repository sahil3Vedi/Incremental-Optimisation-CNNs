{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Incremental Optimisation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP53fiM6jL2UFf5Ek9VVCHB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahil3Vedi/Incremental-Optimisation-CNNs/blob/main/Incremental_Optimisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLrJSdPS_3yC"
      },
      "source": [
        "'''\n",
        "Implementing LeNet 5 on the MNIST dataset using Keras\n",
        "'''\n",
        "# Credits to https://github.com/vaibhavcodes/DeepLearning-Architectures for providing the starter code\n",
        "\n",
        "# Importing Libraries\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, AvgPool2D, Flatten, Dense\n",
        "#from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam \n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "# Image processing & plotting\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2 as cv"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG3pJBfOAMVb"
      },
      "source": [
        "# Loading the dataset and performing train-test split\n",
        "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
        "TRAIN_X = []\n",
        "kernel = np.ones((5,5),np.float32)/25\n",
        "for img in train_x:\n",
        "  sharp = img.reshape(28,28)\n",
        "  dst = cv.filter2D(sharp,-1,kernel)\n",
        "  TRAIN_X.append(dst)\n",
        "\n",
        "TRAIN_X = np.array(TRAIN_X)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nMZV_JECrNp",
        "outputId": "26624a75-cf6b-4872-af39-ca4e82e8f209"
      },
      "source": [
        "print(np.shape(train_x))\n",
        "print(np.shape(TRAIN_X))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(60000, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTh4pHpoCBvk",
        "outputId": "0a385962-1bde-466c-a157-6f937c18bf1a"
      },
      "source": [
        "# Checking the sizes of train and test split\n",
        "print(\"The size of TRIAN_X is: {}\".format(TRAIN_X.shape))\n",
        "print(\"The size of train_x is: {}\".format(train_x.shape))\n",
        "print(\"The size of train_y is: {}\".format(train_y.shape))\n",
        "print(\"The size of test_x is: {}\".format(test_x.shape))\n",
        "print(\"The size of test_y is: {}\".format(test_y.shape))\n",
        "\n",
        "# Performing reshaping operations = Converting into 4D\n",
        "TRAIN_X = TRAIN_X.reshape(TRAIN_X.shape[0], 28, 28, 1)\n",
        "train_x = train_x.reshape(train_x.shape[0], 28, 28, 1)\n",
        "test_x = test_x.reshape(test_x.shape[0], 28, 28, 1)\n",
        "\n",
        "# Normalizing the values of the image- Converting in between 0 and 1\n",
        "TRAIN_X = TRAIN_X/255.0\n",
        "train_x = train_x/255.0\n",
        "test_x = test_x/255.0\n",
        "\n",
        "# One-hot encoding the labels\n",
        "train_y = to_categorical(train_y, num_classes=10)\n",
        "test_y = to_categorical(test_y, num_classes=10)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The size of TRIAN_X is: (60000, 28, 28)\n",
            "The size of train_x is: (60000, 28, 28)\n",
            "The size of train_y is: (60000,)\n",
            "The size of test_x is: (10000, 28, 28)\n",
            "The size of test_y is: (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgelWjFOAQKY",
        "outputId": "7cedc680-3560-4dae-c4f7-cab776e258a3"
      },
      "source": [
        "# Building the Model Architecture\n",
        "\n",
        "# Instanciate an empty model\n",
        "model = Sequential()\n",
        "\n",
        "# Adding a Convolution Layer C1\n",
        "# Input shape = N = (28 x 28)\n",
        "# No. of filters  = 6\n",
        "# Filter size = f = (5 x 5)\n",
        "# Padding = P = 0\n",
        "# Strides = S = 1\n",
        "# Size of each feature map in C1 is (N-f+2P)/S +1 = 28-5+1 = 24\n",
        "# No. of parameters between input layer and C1 = (5*5 + 1)*6 = 156\n",
        "model.add(Conv2D(filters=6, kernel_size=(5,5), padding='valid', input_shape=(28,28,1), activation='tanh'))\n",
        "\n",
        "# Adding an Average Pooling Layer S2\n",
        "# Input shape = N = (24 x 24)\n",
        "# No. of filters = 6\n",
        "# Filter size = f = (2 x 2)\n",
        "# Padding = P = 0\n",
        "# Strides = S = 2\n",
        "# Size of each feature map in S2 is (N-f+2P)/S +1 = (24-2+0)/2+1 = 11+1 = 12\n",
        "# No. of parameters between C1 and S2 = (1+1)*6 = 12\n",
        "model.add(AvgPool2D(pool_size=(2,2)))\n",
        "\n",
        "# Adding a Convolution Layer C3\n",
        "# Input shape = N = (12 x 12)\n",
        "# No. of filters  = 16\n",
        "# Filter size = f = (5 x 5)\n",
        "# Padding = P = 0\n",
        "# Strides = S = 1\n",
        "# Size of each feature map in C3 is (N-f+2P)/S +1 = 12-5+1 = 8\n",
        "# No. of parameters between S2 and C3 = (5*5*6*16 + 16) + 16 = 2416\n",
        "model.add(Conv2D(filters=16, kernel_size=(5,5), padding='valid', activation='tanh'))\n",
        "\n",
        "# Adding an Average Pooling Layer S4\n",
        "# Input shape = N = (8 x 8)\n",
        "# No. of filters = 16\n",
        "# Filter size = f = (2 x 2)\n",
        "# Padding = P = 0\n",
        "# Strides = S = 2\n",
        "# Size of each feature map in S4 is (N-f+2P)/S +1 = (8-2+0)/2+1 = 3+1 = 4\n",
        "# No. of parameters between C3 and S4 = (1+1)*16 = 32\n",
        "model.add(AvgPool2D(pool_size=(2,2)))\n",
        "\n",
        "# As compared to LeNet-5 architecture there was one more application of convolution but in our code  further application of \n",
        "# convolution with (5 x 5) filter would result in a negative dimension which is not possible. So we aren't applying any more\n",
        "# convolution here.\n",
        "\n",
        "# Flattening the layer S4\n",
        "# There would be 16*(4*4) = 256 neurons\n",
        "model.add(Flatten())\n",
        "\n",
        "# Adding a Dense layer with `tanh` activation+# \n",
        "# No. of inputs = 256\n",
        "# No. of outputs = 120\n",
        "# No. of parameters = 256*120 + 120 = 30,840\n",
        "model.add(Dense(120, activation='tanh'))\n",
        "\n",
        "# Adding a Dense layer with `tanh` activation\n",
        "# No. of inputs = 120\n",
        "# No. of outputs = 84\n",
        "# No. of parameters = 120*84 + 84 = 10,164\n",
        "model.add(Dense(84, activation='tanh'))\n",
        "\n",
        "# Adding a Dense layer with `softmax` activation\n",
        "# No. of inputs = 84\n",
        "# No. of outputs = 10\n",
        "# No. of parameters = 84*10 + 10 = 850\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 24, 24, 6)         156       \n",
            "_________________________________________________________________\n",
            "average_pooling2d_6 (Average (None, 12, 12, 6)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 8, 8, 16)          2416      \n",
            "_________________________________________________________________\n",
            "average_pooling2d_7 (Average (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 120)               30840     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 84)                10164     \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                850       \n",
            "=================================================================\n",
            "Total params: 44,426\n",
            "Trainable params: 44,426\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INtJ_RNwAxPJ",
        "outputId": "8538db4a-d8b6-44b8-86e4-9a38401c60b5"
      },
      "source": [
        "# Compiling the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "model.fit(TRAIN_X, train_y, batch_size=128, epochs=5, verbose=1, validation_data=(test_x, test_y))\n",
        "model.fit(train_x, train_y, batch_size=128, epochs=15, verbose=1, validation_data=(test_x, test_y))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 4s 7ms/step - loss: 0.3978 - accuracy: 0.8818 - val_loss: 0.1978 - val_accuracy: 0.9381\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1584 - accuracy: 0.9512 - val_loss: 0.1489 - val_accuracy: 0.9514\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.1126 - accuracy: 0.9650 - val_loss: 0.1316 - val_accuracy: 0.9577\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0882 - accuracy: 0.9730 - val_loss: 0.1395 - val_accuracy: 0.9556\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0744 - accuracy: 0.9766 - val_loss: 0.1088 - val_accuracy: 0.9649\n",
            "Epoch 1/15\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0589 - accuracy: 0.9816 - val_loss: 0.0595 - val_accuracy: 0.9811\n",
            "Epoch 2/15\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0480 - accuracy: 0.9854 - val_loss: 0.0583 - val_accuracy: 0.9807\n",
            "Epoch 3/15\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0395 - accuracy: 0.9878 - val_loss: 0.0566 - val_accuracy: 0.9809\n",
            "Epoch 4/15\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0355 - accuracy: 0.9891 - val_loss: 0.0539 - val_accuracy: 0.9841\n",
            "Epoch 5/15\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0286 - accuracy: 0.9911 - val_loss: 0.0543 - val_accuracy: 0.9830\n",
            "Epoch 6/15\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0265 - accuracy: 0.9918 - val_loss: 0.0494 - val_accuracy: 0.9841\n",
            "Epoch 7/15\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0219 - accuracy: 0.9936 - val_loss: 0.0503 - val_accuracy: 0.9838\n",
            "Epoch 8/15\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0193 - accuracy: 0.9941 - val_loss: 0.0513 - val_accuracy: 0.9854\n",
            "Epoch 9/15\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0172 - accuracy: 0.9947 - val_loss: 0.0468 - val_accuracy: 0.9862\n",
            "Epoch 10/15\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0148 - accuracy: 0.9959 - val_loss: 0.0544 - val_accuracy: 0.9840\n",
            "Epoch 11/15\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0116 - accuracy: 0.9969 - val_loss: 0.0487 - val_accuracy: 0.9866\n",
            "Epoch 12/15\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0103 - accuracy: 0.9970 - val_loss: 0.0492 - val_accuracy: 0.9850\n",
            "Epoch 13/15\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0100 - accuracy: 0.9968 - val_loss: 0.0505 - val_accuracy: 0.9859\n",
            "Epoch 14/15\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0108 - accuracy: 0.9966 - val_loss: 0.0495 - val_accuracy: 0.9863\n",
            "Epoch 15/15\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0070 - accuracy: 0.9980 - val_loss: 0.0495 - val_accuracy: 0.9873\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbcfc9d2450>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mINGkpHHBW6",
        "outputId": "fd42d651-dc85-410b-92ed-528efdf2a9b3"
      },
      "source": [
        "# Finding the loss and accuracy of the model\n",
        "score = model.evaluate(test_x, test_y)\n",
        "\n",
        "print('Test Loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0495 - accuracy: 0.9873\n",
            "Test Loss: 0.049491699784994125\n",
            "Test accuracy: 0.9872999787330627\n"
          ]
        }
      ]
    }
  ]
}